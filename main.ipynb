{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does Enhancing Images also Enhance the Outcome of Image Recognition of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Layout for this program\n",
    "\n",
    "1. Imports\n",
    "2. Load the training set\n",
    "3. Load in the test set\n",
    "4. Load in the test set labels\n",
    "5. Create a copy of the image set and apply CLAHE to it\n",
    "6. Load in 2 pretrained resnets\n",
    "7. Train resnet1 on OG images, train resnet2 on CLAHE images\n",
    "8. Compare performance of both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import *\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize ImageNet so there are 50 images from each catagory in a test folder\n",
    "\n",
    "-> All under imageNet/ILSVRC/Data/CLS-LOC/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopy(f \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m file, headDirectory \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtest/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m folder)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/shutil.py:427\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(dst):\n\u001b[1;32m    426\u001b[0m     dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(src))\n\u001b[0;32m--> 427\u001b[0m copyfile(src, dst, follow_symlinks\u001b[39m=\u001b[39;49mfollow_symlinks)\n\u001b[1;32m    428\u001b[0m copymode(src, dst, follow_symlinks\u001b[39m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/shutil.py:270\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[1;32m    269\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m         _fastcopy_fcopyfile(fsrc, fdst, posix\u001b[39m.\u001b[39;49m_COPYFILE_DATA)\n\u001b[1;32m    271\u001b[0m         \u001b[39mreturn\u001b[39;00m dst\n\u001b[1;32m    272\u001b[0m     \u001b[39mexcept\u001b[39;00m _GiveupOnFastCopy:\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/shutil.py:105\u001b[0m, in \u001b[0;36m_fastcopy_fcopyfile\u001b[0;34m(fsrc, fdst, flags)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[39mraise\u001b[39;00m _GiveupOnFastCopy(err)  \u001b[39m# not a regular file\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     posix\u001b[39m.\u001b[39;49m_fcopyfile(infd, outfd, flags)\n\u001b[1;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    107\u001b[0m     err\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m fsrc\u001b[39m.\u001b[39mname\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "headDirectory = './imageNet/ILSVRC/Data/CLS-LOC/'\n",
    "\n",
    "if os.path.isdir('./imageNet/ILSVRC/Data/CLS-LOC/' + 'test'):\n",
    "    shutil.rmtree(headDirectory + 'test')\n",
    "    \n",
    "os.mkdir(headDirectory + 'test')\n",
    "\n",
    "for folder in sorted(os.listdir(headDirectory + 'train')):\n",
    "\n",
    "    if (folder == 'sals' or folder == '.DS_Store'):\n",
    "        continue\n",
    "    \n",
    "    f = os.path.join(headDirectory + 'train/', folder)\n",
    "    os.mkdir(headDirectory + 'test/' + folder)\n",
    "    counter = 0\n",
    "\n",
    "    for file in sorted( os.listdir(f)):\n",
    "        #print(file)\n",
    "        if counter == 50:\n",
    "            break\n",
    "        else:\n",
    "            shutil.copy(f + '/' + file, headDirectory + 'test/' + folder)\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Spotted Salamander image set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size fo the largest and smallest images in the spotted salamander set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 1150\n"
     ]
    }
   ],
   "source": [
    "smallestHeight = 1000\n",
    "smallestWidth = 1000\n",
    "largestHeight = 0\n",
    "largestWidth = 0\n",
    "numImages = 0\n",
    "\n",
    "directory = './imageNet/ILSVRC/Data/CLS-LOC/train/n01632458'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        numImages += 1\n",
    "        img = cv2.imread(f)\n",
    "\n",
    "        if img.shape[0] < smallestHeight:\n",
    "            smallestHeight = img.shape[0]\n",
    "        if img.shape[1] < smallestWidth:\n",
    "            smallestWidth = img.shape[1]\n",
    "        if img.shape[0] > largestHeight:\n",
    "            largestHeight = img.shape[0]\n",
    "        if img.shape[1] > largestWidth:\n",
    "            largestWidth = img.shape[1]\n",
    "\n",
    "print(\"Number of Images: \" + str(numImages))\n",
    "# print('Smallest Height = ' + str(smallestHeight))\n",
    "# print('Smallest Width = ' + str(smallestWidth))\n",
    "# print('Largest Height = ' + str(largestHeight))\n",
    "# print('Largest Width = ' + str(largestWidth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the images in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes =  1001\n",
      "<class 'list'>\n",
      "n01440764\n",
      "len =  1132059\n",
      "item =  (tensor([[[0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         ...,\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764]],\n",
      "\n",
      "        [[0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         ...,\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764]],\n",
      "\n",
      "        [[0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         ...,\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764],\n",
      "         [0.8764, 0.8764, 0.8764,  ..., 0.8764, 0.8764, 0.8764]]]), 0)\n",
      "<class 'torchvision.datasets.folder.ImageFolder'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import numpy\n",
    "from torchvision import transforms\n",
    "transforms = transforms.Compose(\n",
    "[\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Resize((smallestHeight, smallestWidth)),\n",
    "   transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "])\n",
    "\n",
    "directory = 'imageNet/ILSVRC/Data/CLS-LOC/train'\n",
    "train_dataset = torchvision.datasets.ImageFolder(directory, transforms)\n",
    "num_classes = len(train_dataset.classes)\n",
    "print('num classes = ', len(train_dataset.classes))\n",
    "print(type(train_dataset.classes))\n",
    "\n",
    "print(train_dataset.classes[0])\n",
    "\n",
    "# Need to remove first 50 images from each class\n",
    "#train_dataset = torch.utils.data.Subset(train_dataset, range(50, len(train_dataset)))\n",
    "\n",
    "\n",
    "print('len = ', train_dataset.__len__())\n",
    "print('item = ', train_dataset.__getitem__(0))\n",
    "\n",
    "print(type(train_dataset))\n",
    "print(type([train_dataset[0]]))\n",
    "print(type([train_dataset[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesnt show images right now\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the loaded images into a dataloader datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3797, -1.3058,  2.1702,  1.1050,  0.6462],\n",
      "        [ 0.8842,  0.2101, -1.9627,  1.5388,  0.0451],\n",
      "        [ 0.1291, -0.9840, -0.0348, -0.7895, -0.7323]], requires_grad=True)\n",
      "tensor([2, 4, 0])\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x2cba6c190>\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT, num_classes=num_classes - 1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(resnet.parameters())\n",
    "\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 128)\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "len targets 100\n",
      "len outputs 100\n",
      "target_ tensor([333, 934, 485, 266, 709,   7, 706, 355, 331, 263, 675, 616, 987, 606,\n",
      "        664, 624, 520, 908, 350, 828, 799, 277, 955, 467, 712, 280, 877, 990,\n",
      "        768, 804, 471, 692, 219, 518, 388, 388, 267, 290, 732, 308,  49, 237,\n",
      "        184, 839, 263, 341, 879, 287, 944, 594, 662, 512,  40, 633, 450, 977,\n",
      "        283, 480, 896, 842, 943, 815, 788, 291, 759, 966, 450, 646, 931, 799,\n",
      "        542, 954, 114,   0,  26, 945,  80, 648, 858, 390, 842, 892, 968, 203,\n",
      "        381, 834,  45, 604, 362, 425, 886, 608,  62, 769, 195,  72, 176, 705,\n",
      "        399, 692])\n",
      "outputs tensor([[-1.0648, -0.5896, -0.6819,  ..., -0.3720,  0.2740, -0.3472],\n",
      "        [-0.0666, -0.8909,  0.2444,  ...,  0.1824,  0.3586,  0.5651],\n",
      "        [ 0.1070,  0.8505,  0.0860,  ..., -1.1745,  1.6643,  0.1102],\n",
      "        ...,\n",
      "        [-0.6421, -1.1351,  1.2851,  ..., -0.2563,  0.6475, -0.5935],\n",
      "        [-0.9007, -1.8831,  1.2034,  ..., -0.5255,  0.5767,  0.6071],\n",
      "        [-0.3760, -0.2237, -0.1907,  ..., -0.6353, -0.6921,  1.0273]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 333 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtarget_\u001b[39m\u001b[39m\"\u001b[39m, target_)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, outputs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X31sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, target_)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X31sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#X31sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1147\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1146\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1148\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/loss.py:1166\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1167\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1168\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/functional.py:3012\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3011\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3012\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 333 is out of bounds."
     ]
    }
   ],
   "source": [
    "from cgitb import reset\n",
    "\n",
    "\n",
    "n_epochs = 5\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_loader)\n",
    "\n",
    "resnet.train()\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    batch_idx = -1\n",
    "    for i, (data_, target_) in enumerate(train_loader):\n",
    "        batch_idx += 1\n",
    "\n",
    "\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = resnet(data_)\n",
    "        print(\"len targets\", len(target_))\n",
    "        print(\"len outputs\", len(outputs))\n",
    "\n",
    "        print(\"target_\", target_)\n",
    "        print(\"outputs\", outputs)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t = 0\n",
    "    correct_t = 0\n",
    "    with torch.no_grad():\n",
    "        resnet.eval()\n",
    "        for data_t, target_t in (train_loader):\n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = resnet(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(train_loader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "\n",
    "        \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(resnet.state_dict(), 'resnet.pt')\n",
    "            print('Improvement-Detected, save-model')\n",
    "    resnet.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'SalamanderResnet.pth'\n",
    "torch.save(resnet.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch into eval mode and test it on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.Subset'>\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from cgi import test\n",
    "\n",
    "resnet.eval()\n",
    "\n",
    "testSet = []\n",
    "directory = './imageNet/ILSVRC/Data/CLS-LOC/test'\n",
    "\n",
    "\n",
    "#Load in all test salamader images\n",
    "testData = torchvision.datasets.ImageFolder(directory, transforms)\n",
    "testData = torch.utils.data.Subset(testData, range(50))\n",
    "\n",
    "print(type(testData))\n",
    "print(len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "inputs =  tensor([[[[0.5744, 0.3828, 0.2692,  ..., 0.1600, 0.0672, 0.5370],\n",
      "          [0.4968, 0.3041, 0.2234,  ..., 0.0667, 0.0786, 0.3704],\n",
      "          [0.5122, 0.2530, 0.4054,  ..., 0.1577, 0.1067, 0.1071],\n",
      "          ...,\n",
      "          [0.2044, 0.3462, 0.3323,  ..., 0.3689, 0.2107, 0.2716],\n",
      "          [0.2433, 0.5014, 0.2148,  ..., 0.2467, 0.1235, 0.0477],\n",
      "          [0.5387, 0.4669, 0.3714,  ..., 0.3030, 0.2539, 0.1776]],\n",
      "\n",
      "         [[0.6834, 0.5164, 0.2964,  ..., 0.2198, 0.2356, 0.6205],\n",
      "          [0.5812, 0.3899, 0.2398,  ..., 0.1474, 0.1986, 0.4012],\n",
      "          [0.6164, 0.3831, 0.5370,  ..., 0.2269, 0.1961, 0.1569],\n",
      "          ...,\n",
      "          [0.3078, 0.4447, 0.4244,  ..., 0.4391, 0.3629, 0.4134],\n",
      "          [0.3622, 0.5387, 0.2696,  ..., 0.4215, 0.2722, 0.5032],\n",
      "          [0.6350, 0.5920, 0.4177,  ..., 0.3695, 0.4205, 0.3388]],\n",
      "\n",
      "         [[0.7223, 0.4177, 0.2235,  ..., 0.0844, 0.0559, 0.5114],\n",
      "          [0.5747, 0.2574, 0.1546,  ..., 0.0447, 0.0674, 0.2897],\n",
      "          [0.4645, 0.1678, 0.2778,  ..., 0.1316, 0.0684, 0.0997],\n",
      "          ...,\n",
      "          [0.1069, 0.2848, 0.3481,  ..., 0.3992, 0.2724, 0.3360],\n",
      "          [0.2326, 0.5300, 0.1174,  ..., 0.3472, 0.2017, 0.1575],\n",
      "          [0.5435, 0.4730, 0.3355,  ..., 0.3450, 0.3265, 0.2301]]],\n",
      "\n",
      "\n",
      "        [[[0.0280, 0.0322, 0.3081,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.0397, 0.0949, 0.2702,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.0320, 0.0059, 0.0368,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0843, 0.1116, 0.1249],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0697, 0.1556, 0.1033],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1021, 0.2368, 0.1154]],\n",
      "\n",
      "         [[0.1643, 0.1618, 0.4982,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.1131, 0.1618, 0.2699,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.0656, 0.1111, 0.2495,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.0029, 0.0033, 0.0000,  ..., 0.1895, 0.2221, 0.1836],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1308, 0.2745, 0.1806],\n",
      "          [0.0000, 0.0082, 0.0018,  ..., 0.2613, 0.3208, 0.1079]],\n",
      "\n",
      "         [[0.0076, 0.0283, 0.1825,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.0328, 0.1383, 0.4515,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.0284, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.0020, 0.0024, 0.0000,  ..., 0.0859, 0.0403, 0.0780],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0520, 0.1464, 0.0787],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0634, 0.2660, 0.0752]]],\n",
      "\n",
      "\n",
      "        [[[0.4650, 0.2482, 0.2484,  ..., 0.0794, 0.0929, 0.0891],\n",
      "          [0.1777, 0.2535, 0.2678,  ..., 0.0819, 0.0769, 0.0888],\n",
      "          [0.1781, 0.2470, 0.1003,  ..., 0.0919, 0.0937, 0.0869],\n",
      "          ...,\n",
      "          [0.1244, 0.1936, 0.1267,  ..., 0.1331, 0.1294, 0.1142],\n",
      "          [0.1931, 0.1492, 0.2052,  ..., 0.1154, 0.1133, 0.0982],\n",
      "          [0.1230, 0.1885, 0.1259,  ..., 0.1273, 0.0960, 0.1045]],\n",
      "\n",
      "         [[0.5909, 0.3480, 0.3358,  ..., 0.0894, 0.1041, 0.0991],\n",
      "          [0.2401, 0.3548, 0.3675,  ..., 0.0919, 0.0867, 0.0988],\n",
      "          [0.2731, 0.3000, 0.1331,  ..., 0.1044, 0.1052, 0.0964],\n",
      "          ...,\n",
      "          [0.1583, 0.2218, 0.1696,  ..., 0.1377, 0.1317, 0.1242],\n",
      "          [0.2401, 0.1889, 0.2450,  ..., 0.1232, 0.1194, 0.1082],\n",
      "          [0.1529, 0.2288, 0.1833,  ..., 0.1373, 0.1175, 0.1074]],\n",
      "\n",
      "         [[0.4365, 0.1977, 0.1350,  ..., 0.0987, 0.1160, 0.1084],\n",
      "          [0.1511, 0.1754, 0.1635,  ..., 0.1012, 0.1078, 0.1081],\n",
      "          [0.1639, 0.1869, 0.0750,  ..., 0.1232, 0.1153, 0.1048],\n",
      "          ...,\n",
      "          [0.1745, 0.2255, 0.1763,  ..., 0.1470, 0.1298, 0.1335],\n",
      "          [0.2371, 0.2039, 0.2451,  ..., 0.1279, 0.1129, 0.1175],\n",
      "          [0.1572, 0.2279, 0.1799,  ..., 0.1466, 0.1115, 0.1233]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.2048, 0.2318, 0.3309,  ..., 0.2621, 0.2112, 0.3160],\n",
      "          [0.1744, 0.3763, 0.3414,  ..., 0.1795, 0.3429, 0.2581],\n",
      "          [0.3410, 0.3152, 0.3103,  ..., 0.2118, 0.2976, 0.2947],\n",
      "          ...,\n",
      "          [0.0871, 0.1467, 0.0911,  ..., 0.1826, 0.1105, 0.1865],\n",
      "          [0.1343, 0.1544, 0.0846,  ..., 0.1211, 0.0933, 0.0893],\n",
      "          [0.1404, 0.1389, 0.1633,  ..., 0.1172, 0.1346, 0.2027]],\n",
      "\n",
      "         [[0.1623, 0.1923, 0.3156,  ..., 0.2595, 0.2229, 0.3222],\n",
      "          [0.1494, 0.3525, 0.3397,  ..., 0.1703, 0.3459, 0.2628],\n",
      "          [0.3384, 0.3126, 0.3079,  ..., 0.2105, 0.2963, 0.3093],\n",
      "          ...,\n",
      "          [0.0824, 0.1599, 0.1221,  ..., 0.2051, 0.1363, 0.2377],\n",
      "          [0.1296, 0.1676, 0.1156,  ..., 0.1569, 0.1376, 0.0672],\n",
      "          [0.1487, 0.1471, 0.1703,  ..., 0.1122, 0.1430, 0.2458]],\n",
      "\n",
      "         [[0.1200, 0.0822, 0.2159,  ..., 0.1516, 0.0805, 0.0424],\n",
      "          [0.0589, 0.1975, 0.1937,  ..., 0.0447, 0.1766, 0.0413],\n",
      "          [0.1513, 0.1254, 0.1191,  ..., 0.0330, 0.1188, 0.1132],\n",
      "          ...,\n",
      "          [0.0688, 0.1348, 0.0852,  ..., 0.1379, 0.0801, 0.0829],\n",
      "          [0.1161, 0.1425, 0.0787,  ..., 0.1090, 0.0902, 0.0722],\n",
      "          [0.1268, 0.1253, 0.1448,  ..., 0.0484, 0.1205, 0.1234]]],\n",
      "\n",
      "\n",
      "        [[[0.4488, 0.4088, 0.3588,  ..., 0.5396, 0.6104, 0.5160],\n",
      "          [0.4077, 0.4251, 0.3825,  ..., 0.3487, 0.3639, 0.3974],\n",
      "          [0.4916, 0.4304, 0.3424,  ..., 0.2431, 0.2603, 0.3569],\n",
      "          ...,\n",
      "          [0.3558, 0.3108, 0.2574,  ..., 0.3663, 0.2927, 0.3597],\n",
      "          [0.2039, 0.2778, 0.2515,  ..., 0.3914, 0.3721, 0.4001],\n",
      "          [0.3876, 0.4346, 0.3192,  ..., 0.3128, 0.4356, 0.4905]],\n",
      "\n",
      "         [[0.5291, 0.5399, 0.4665,  ..., 0.5515, 0.5645, 0.5371],\n",
      "          [0.5024, 0.5171, 0.4829,  ..., 0.4767, 0.4794, 0.5026],\n",
      "          [0.5458, 0.4943, 0.4327,  ..., 0.3855, 0.3930, 0.4650],\n",
      "          ...,\n",
      "          [0.5427, 0.4186, 0.4359,  ..., 0.4639, 0.4283, 0.4314],\n",
      "          [0.3462, 0.3677, 0.4081,  ..., 0.3752, 0.3348, 0.4509],\n",
      "          [0.5207, 0.5461, 0.4572,  ..., 0.3850, 0.4565, 0.5284]],\n",
      "\n",
      "         [[0.3719, 0.4089, 0.3623,  ..., 0.4619, 0.5168, 0.4480],\n",
      "          [0.3527, 0.3673, 0.3432,  ..., 0.2891, 0.3089, 0.3520],\n",
      "          [0.3673, 0.3151, 0.3039,  ..., 0.2868, 0.3132, 0.3702],\n",
      "          ...,\n",
      "          [0.1223, 0.1141, 0.1007,  ..., 0.1155, 0.1223, 0.1927],\n",
      "          [0.0406, 0.1429, 0.0607,  ..., 0.1692, 0.1473, 0.2424],\n",
      "          [0.2233, 0.2143, 0.1124,  ..., 0.1087, 0.1848, 0.2234]]],\n",
      "\n",
      "\n",
      "        [[[0.2807, 0.1842, 0.3304,  ..., 0.9159, 0.9147, 0.9156],\n",
      "          [0.2819, 0.4271, 0.3215,  ..., 0.9086, 0.7002, 0.6774],\n",
      "          [0.0967, 0.4445, 0.5544,  ..., 0.5876, 0.5440, 0.5094],\n",
      "          ...,\n",
      "          [0.4869, 0.3870, 0.4316,  ..., 0.5078, 0.5456, 0.3265],\n",
      "          [0.3477, 0.3898, 0.3952,  ..., 0.2725, 0.4245, 0.3077],\n",
      "          [0.4827, 0.5035, 0.3737,  ..., 0.6371, 0.4819, 0.0682]],\n",
      "\n",
      "         [[0.3965, 0.2876, 0.3912,  ..., 0.9159, 0.9105, 0.9058],\n",
      "          [0.3830, 0.5098, 0.3985,  ..., 0.9079, 0.7943, 0.7419],\n",
      "          [0.1373, 0.5313, 0.6329,  ..., 0.6723, 0.6518, 0.6218],\n",
      "          ...,\n",
      "          [0.6146, 0.4793, 0.5635,  ..., 0.5268, 0.5943, 0.3994],\n",
      "          [0.4615, 0.4953, 0.5119,  ..., 0.2876, 0.4505, 0.3694],\n",
      "          [0.6153, 0.5961, 0.4749,  ..., 0.6379, 0.4806, 0.0552]],\n",
      "\n",
      "         [[0.3061, 0.1718, 0.2727,  ..., 0.9159, 0.9013, 0.9110],\n",
      "          [0.2643, 0.4391, 0.2702,  ..., 0.9011, 0.7089, 0.6172],\n",
      "          [0.1284, 0.3916, 0.4810,  ..., 0.5845, 0.5841, 0.5497],\n",
      "          ...,\n",
      "          [0.4871, 0.2589, 0.3267,  ..., 0.3203, 0.3073, 0.2367],\n",
      "          [0.3642, 0.3192, 0.3351,  ..., 0.1109, 0.3131, 0.2987],\n",
      "          [0.4872, 0.4675, 0.2588,  ..., 0.4139, 0.3568, 0.0535]]]])\n",
      "loop 0\n",
      "outputs  tensor([[ 0.6398,  0.0137,  1.2117,  ..., -0.7115,  0.1335, -0.9083],\n",
      "        [ 0.5855, -0.4628,  0.2170,  ...,  0.0878, -1.0861, -0.3965],\n",
      "        [ 0.8954,  0.0135,  0.0355,  ...,  0.4094, -0.4731, -0.7885],\n",
      "        ...,\n",
      "        [ 1.4880, -0.5634,  0.4472,  ...,  1.2546, -0.6701, -0.6724],\n",
      "        [ 0.5056, -0.2816,  0.2645,  ...,  0.1677, -1.1056, -1.0383],\n",
      "        [ 0.5717, -0.1550,  0.4040,  ..., -0.1868, -0.6153, -0.8571]])\n",
      "======>RESULTS<======\n",
      "[Test #1] Loss: 4.5177 Acc: 0.0000%\n"
     ]
    }
   ],
   "source": [
    "#Switch Model into eval mode\n",
    "resnet.eval()\n",
    "test_loader = DataLoader(testData, batch_size=50, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        print(\"label = \", labels)\n",
    "        print(\"inputs = \", inputs)\n",
    "        print(\"loop \" + str(i))\n",
    "        if (i == 50):\n",
    "            break\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = resnet(inputs)\n",
    "        print('outputs ', outputs )\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        if i == 0:\n",
    "            print('======>RESULTS<======')\n",
    "            #images = torchvision.utils.make_grid(inputs[:4])\n",
    "    epoch_loss = running_loss / len(testData)\n",
    "    epoch_acc = running_corrects / len(testData) * 100.\n",
    "    print('[Test #{}] Loss: {:.4f} Acc: {:.4f}%'.\n",
    "          format(epoch, epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a779665a45765164b4f9c0a2c1367197c2c065231701c06d3ada1eb196bfe45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
