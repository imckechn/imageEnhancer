{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does Enhancing Images also Enhance the Outcome of Image Recognition of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Layout for this program\n",
    "\n",
    "1. Imports\n",
    "2. Load the training set\n",
    "3. Load in the test set\n",
    "4. Load in the test set labels\n",
    "5. Create a copy of the image set and apply CLAHE to it\n",
    "6. Load in 2 pretrained resnets\n",
    "7. Train resnet1 on OG images, train resnet2 on CLAHE images\n",
    "8. Compare performance of both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import *\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ian/miniforge3/envs/ml/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ian/miniforge3/envs/ml/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)   #load resnet18 model\n",
    "num_features = model.fc.in_features     #extract fc layers features\n",
    "model.fc = nn.Linear(num_features, 2) #(num_of_class == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize ImageNet so there are 50 images from each catagory in a test folder\n",
    "\n",
    "-> All under imageNet/ILSVRC/Data/CLS-LOC/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import constants\n",
    "import shutil\n",
    "\n",
    "headDirectory = './imageNet/ILSVRC/Data/CLS-LOC/'\n",
    "\n",
    "if not os.path.isdir('./imageNet/ILSVRC/Data/CLS-LOC/' + 'test'):\n",
    "    os.mkdir(headDirectory + 'test')\n",
    "else:\n",
    "    shutil.rmtree(headDirectory + 'test')\n",
    "    os.mkdir(headDirectory + 'test')\n",
    "\n",
    "for folder in os.listdir(headDirectory + 'train'):\n",
    "\n",
    "    if (folder == 'sals' or folder == '.DS_Store'):\n",
    "        continue\n",
    "    \n",
    "    f = os.path.join(headDirectory + 'train/', folder)\n",
    "    os.mkdir(headDirectory + 'test/' + folder)\n",
    "    counter = 0\n",
    "\n",
    "    for file in sorted( os.listdir(f)):\n",
    "        if counter == 50:\n",
    "            break\n",
    "        else:\n",
    "            shutil.copy(f + '/' + file, headDirectory + 'test/' + folder)\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the ImageNet Spotted Salamanders\n",
    "Loop through the imagenet training directory pulling in all 1200 Spotted Salamader images. Then create a copy of the images and label it CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 1150\n"
     ]
    }
   ],
   "source": [
    "smallestHeight = 1000\n",
    "smallestWidth = 1000\n",
    "largestHeight = 0\n",
    "largestWidth = 0\n",
    "numImages = 0\n",
    "\n",
    "directory = './imageNet/ILSVRC/Data/CLS-LOC/train/n01632458'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        numImages += 1\n",
    "        img = cv2.imread(f)\n",
    "\n",
    "        if img.shape[0] < smallestHeight:\n",
    "            smallestHeight = img.shape[0]\n",
    "        if img.shape[1] < smallestWidth:\n",
    "            smallestWidth = img.shape[1]\n",
    "        if img.shape[0] > largestHeight:\n",
    "            largestHeight = img.shape[0]\n",
    "        if img.shape[1] > largestWidth:\n",
    "            largestWidth = img.shape[1]\n",
    "\n",
    "print(\"Number of Images: \" + str(numImages))\n",
    "# print('Smallest Height = ' + str(smallestHeight))\n",
    "# print('Smallest Width = ' + str(smallestWidth))\n",
    "# print('Largest Height = ' + str(largestHeight))\n",
    "# print('Largest Width = ' + str(largestWidth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n",
      "<class 'torch.utils.data.dataset.Subset'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import numpy\n",
    "from torchvision import transforms\n",
    "transforms = transforms.Compose(\n",
    "[\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Resize((smallestHeight, smallestWidth)),\n",
    "   transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "])\n",
    "\n",
    "directory = 'imageNet/ILSVRC/Data/CLS-LOC/train/sals'\n",
    "train_dataset = torchvision.datasets.ImageFolder(directory, transforms)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, range(50, len(train_dataset)))\n",
    "\n",
    "\n",
    "print(len(train_dataset))\n",
    "\n",
    "print(type(train_dataset))\n",
    "print(type([train_dataset[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet18(pretrained=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    _,pred = torch.max(out, dim=1)\n",
    "    return torch.sum(pred==labels).item()\n",
    "\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 128)\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/5], Step [0/13], Loss: 4.5263\n",
      "\n",
      "train-loss: 4.5532, train-acc: 0.8800\n",
      "validation loss: 4.6035, validation acc: 0.4800\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 2\n",
      "\n",
      "Epoch [2/5], Step [0/13], Loss: 4.5412\n",
      "\n",
      "train-loss: 4.5537, train-acc: 1.3600\n",
      "validation loss: 4.5872, validation acc: 0.8800\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 3\n",
      "\n",
      "Epoch [3/5], Step [0/13], Loss: 4.5567\n",
      "\n",
      "train-loss: 4.5523, train-acc: 0.7200\n",
      "validation loss: 4.5869, validation acc: 0.5600\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "Epoch [4/5], Step [0/13], Loss: 4.5303\n",
      "\n",
      "train-loss: 4.5495, train-acc: 0.8000\n",
      "validation loss: 4.5900, validation acc: 0.5600\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "Epoch [5/5], Step [0/13], Loss: 4.5411\n",
      "\n",
      "train-loss: 4.5453, train-acc: 1.2800\n",
      "validation loss: 4.5853, validation acc: 1.2800\n",
      "\n",
      "Improvement-Detected, save-model\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    batch_idx = -1\n",
    "    for data_, target_ in train_loader:\n",
    "        batch_idx += 1\n",
    "\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data_t, target_t in (train_loader):\n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = net(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(train_loader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "\n",
    "        \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(net.state_dict(), 'resnet.pt')\n",
    "            print('Improvement-Detected, save-model')\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'SalamanderResnet.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch into eval mode and test it on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.folder.ImageFolder'>\n",
      "49999\n"
     ]
    }
   ],
   "source": [
    "from cgi import test\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "testSet = []\n",
    "directory = './imageNet/ILSVRC/Data/CLS-LOC/test'\n",
    "\n",
    "\n",
    "#Load in all test salamader images\n",
    "testData = torchvision.datasets.ImageFolder(directory, transforms)\n",
    "\n",
    "\n",
    "print(type(testData))\n",
    "print(len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  tensor([763, 534, 356,  65,  82, 829, 569, 417, 285, 195,  40, 701, 800, 553,\n",
      "        771, 908, 697, 951, 563, 332, 546, 351, 541, 740, 291, 274, 230, 737,\n",
      "        875, 331, 982, 619, 439, 495, 149, 428, 931, 785, 700, 668, 790, 980,\n",
      "        602, 813, 408, 798, 449, 529, 667, 835])\n",
      "inputs =  tensor([[[[0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          ...,\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026]],\n",
      "\n",
      "         [[0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          ...,\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026]],\n",
      "\n",
      "         [[0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          ...,\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026],\n",
      "          [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026]]],\n",
      "\n",
      "\n",
      "        [[[0.3748, 0.3680, 0.4444,  ..., 0.9046, 0.8729, 0.5363],\n",
      "          [0.3606, 0.3895, 0.4643,  ..., 0.5816, 0.5746, 0.4857],\n",
      "          [0.3838, 0.4147, 0.4795,  ..., 0.4860, 0.4730, 0.6902],\n",
      "          ...,\n",
      "          [0.2688, 0.4111, 0.4220,  ..., 0.6275, 0.6281, 0.5835],\n",
      "          [0.2299, 0.3086, 0.4319,  ..., 0.6013, 0.6100, 0.5991],\n",
      "          [0.2360, 0.2496, 0.3644,  ..., 0.6142, 0.6016, 0.5941]],\n",
      "\n",
      "         [[0.3770, 0.4181, 0.4629,  ..., 0.9414, 0.9054, 0.6986],\n",
      "          [0.3924, 0.4170, 0.4917,  ..., 0.6478, 0.6767, 0.6193],\n",
      "          [0.4169, 0.4301, 0.4946,  ..., 0.6287, 0.6133, 0.7499],\n",
      "          ...,\n",
      "          [0.0950, 0.3548, 0.3708,  ..., 0.6072, 0.6052, 0.5925],\n",
      "          [0.0709, 0.1609, 0.3739,  ..., 0.5991, 0.5966, 0.5824],\n",
      "          [0.0589, 0.0690, 0.3271,  ..., 0.5986, 0.5720, 0.5765]],\n",
      "\n",
      "         [[0.3904, 0.3985, 0.4368,  ..., 0.9757, 0.9409, 0.8191],\n",
      "          [0.3902, 0.4172, 0.4640,  ..., 0.7250, 0.7275, 0.6990],\n",
      "          [0.3985, 0.4242, 0.4661,  ..., 0.6643, 0.6579, 0.8161],\n",
      "          ...,\n",
      "          [0.0331, 0.2841, 0.3239,  ..., 0.5902, 0.5872, 0.5599],\n",
      "          [0.0068, 0.1926, 0.3214,  ..., 0.5586, 0.5750, 0.5669],\n",
      "          [0.0063, 0.0124, 0.2607,  ..., 0.5790, 0.5586, 0.5589]]],\n",
      "\n",
      "\n",
      "        [[[0.4527, 0.6126, 0.5894,  ..., 0.5121, 0.5810, 0.3136],\n",
      "          [0.6550, 0.4167, 0.5387,  ..., 0.5295, 0.5541, 0.4130],\n",
      "          [0.6042, 0.4694, 0.4127,  ..., 0.5067, 0.5937, 0.5086],\n",
      "          ...,\n",
      "          [0.4965, 0.6083, 0.5755,  ..., 0.5864, 0.5822, 0.5187],\n",
      "          [0.4685, 0.6488, 0.5853,  ..., 0.5625, 0.6124, 0.4783],\n",
      "          [0.5119, 0.6694, 0.3738,  ..., 0.6376, 0.6723, 0.5894]],\n",
      "\n",
      "         [[0.3902, 0.5411, 0.5073,  ..., 0.4550, 0.5333, 0.3102],\n",
      "          [0.5885, 0.3481, 0.4746,  ..., 0.4684, 0.4915, 0.3859],\n",
      "          [0.5369, 0.4091, 0.3651,  ..., 0.4418, 0.5164, 0.4549],\n",
      "          ...,\n",
      "          [0.4772, 0.5395, 0.4993,  ..., 0.5554, 0.5328, 0.4650],\n",
      "          [0.4444, 0.5842, 0.5180,  ..., 0.5311, 0.5808, 0.4461],\n",
      "          [0.4840, 0.6139, 0.3243,  ..., 0.5919, 0.6297, 0.5443]],\n",
      "\n",
      "         [[0.3285, 0.4673, 0.4362,  ..., 0.3831, 0.4642, 0.2609],\n",
      "          [0.5008, 0.3258, 0.3908,  ..., 0.3949, 0.4160, 0.3282],\n",
      "          [0.4691, 0.3475, 0.3082,  ..., 0.3636, 0.4330, 0.3840],\n",
      "          ...,\n",
      "          [0.4423, 0.4915, 0.4352,  ..., 0.4802, 0.4826, 0.4174],\n",
      "          [0.3916, 0.5406, 0.4608,  ..., 0.4534, 0.5205, 0.3799],\n",
      "          [0.4400, 0.5808, 0.2826,  ..., 0.5276, 0.5495, 0.4701]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6573, 0.6708, 0.6970,  ..., 0.5457, 0.3779, 0.2966],\n",
      "          [0.7023, 0.7092, 0.7280,  ..., 0.5425, 0.5365, 0.2997],\n",
      "          [0.7280, 0.7168, 0.7427,  ..., 0.5685, 0.4924, 0.3219],\n",
      "          ...,\n",
      "          [0.6040, 0.6118, 0.6230,  ..., 0.5038, 0.5458, 0.5190],\n",
      "          [0.5645, 0.5789, 0.6120,  ..., 0.4735, 0.4905, 0.4905],\n",
      "          [0.5424, 0.5415, 0.5925,  ..., 0.4304, 0.3875, 0.3771]],\n",
      "\n",
      "         [[0.6759, 0.6895, 0.7157,  ..., 0.4257, 0.2602, 0.1637],\n",
      "          [0.7082, 0.7151, 0.7338,  ..., 0.4324, 0.4263, 0.1466],\n",
      "          [0.7294, 0.7182, 0.7441,  ..., 0.4577, 0.3823, 0.1624],\n",
      "          ...,\n",
      "          [0.6036, 0.6101, 0.6213,  ..., 0.5021, 0.5441, 0.5173],\n",
      "          [0.5645, 0.5934, 0.6120,  ..., 0.4735, 0.4888, 0.4890],\n",
      "          [0.5424, 0.5560, 0.5925,  ..., 0.4221, 0.3806, 0.3716]],\n",
      "\n",
      "         [[0.6763, 0.6898, 0.7160,  ..., 0.4665, 0.2702, 0.1650],\n",
      "          [0.7137, 0.7206, 0.7393,  ..., 0.4486, 0.4426, 0.1467],\n",
      "          [0.7356, 0.7244, 0.7503,  ..., 0.4630, 0.4099, 0.1548],\n",
      "          ...,\n",
      "          [0.5812, 0.5820, 0.5926,  ..., 0.4734, 0.5154, 0.4887],\n",
      "          [0.5228, 0.5478, 0.5703,  ..., 0.4318, 0.4601, 0.4613],\n",
      "          [0.5006, 0.5104, 0.5507,  ..., 0.3849, 0.3495, 0.3467]]],\n",
      "\n",
      "\n",
      "        [[[0.9121, 0.7263, 0.6144,  ..., 0.3825, 0.3367, 0.3700],\n",
      "          [0.5986, 0.5764, 0.5717,  ..., 0.3389, 0.3816, 0.3191],\n",
      "          [0.5742, 0.5310, 0.5480,  ..., 0.2643, 0.2463, 0.2515],\n",
      "          ...,\n",
      "          [0.0358, 0.0484, 0.0247,  ..., 0.2549, 0.1696, 0.0870],\n",
      "          [0.0127, 0.0328, 0.0163,  ..., 0.3313, 0.2462, 0.2859],\n",
      "          [0.0154, 0.0000, 0.0195,  ..., 0.9121, 0.4536, 0.5297]],\n",
      "\n",
      "         [[0.8542, 0.5706, 0.5037,  ..., 0.2251, 0.2127, 0.2474],\n",
      "          [0.4874, 0.4572, 0.4477,  ..., 0.2204, 0.2355, 0.2090],\n",
      "          [0.4144, 0.4047, 0.3852,  ..., 0.1951, 0.1735, 0.2096],\n",
      "          ...,\n",
      "          [0.0358, 0.0484, 0.0253,  ..., 0.1575, 0.0949, 0.0664],\n",
      "          [0.0127, 0.0328, 0.0163,  ..., 0.1727, 0.1528, 0.1587],\n",
      "          [0.0154, 0.0000, 0.0195,  ..., 0.0113, 0.1460, 0.0866]],\n",
      "\n",
      "         [[0.4467, 0.2777, 0.2980,  ..., 0.1498, 0.0324, 0.1580],\n",
      "          [0.2913, 0.1949, 0.2197,  ..., 0.1609, 0.1593, 0.1447],\n",
      "          [0.2175, 0.2115, 0.2042,  ..., 0.1246, 0.1753, 0.1356],\n",
      "          ...,\n",
      "          [0.0358, 0.0484, 0.0328,  ..., 0.0478, 0.0519, 0.0568],\n",
      "          [0.0127, 0.0328, 0.0163,  ..., 0.0882, 0.0848, 0.0978],\n",
      "          [0.0154, 0.0000, 0.0195,  ..., 0.1242, 0.0514, 0.0227]]],\n",
      "\n",
      "\n",
      "        [[[0.4337, 0.4139, 0.4072,  ..., 0.4264, 0.4367, 0.4266],\n",
      "          [0.3976, 0.4091, 0.3753,  ..., 0.3839, 0.3928, 0.4315],\n",
      "          [0.3809, 0.4148, 0.4636,  ..., 0.4434, 0.4071, 0.4049],\n",
      "          ...,\n",
      "          [0.3785, 0.3691, 0.3936,  ..., 0.2936, 0.2901, 0.2211],\n",
      "          [0.3444, 0.3030, 0.3488,  ..., 0.2905, 0.2730, 0.2563],\n",
      "          [0.1474, 0.1521, 0.1993,  ..., 0.2981, 0.2765, 0.2441]],\n",
      "\n",
      "         [[0.4036, 0.3963, 0.3772,  ..., 0.4129, 0.4172, 0.4071],\n",
      "          [0.3694, 0.3891, 0.3471,  ..., 0.3638, 0.3682, 0.4069],\n",
      "          [0.3445, 0.3888, 0.4273,  ..., 0.4224, 0.3897, 0.3875],\n",
      "          ...,\n",
      "          [0.3177, 0.3150, 0.3397,  ..., 0.2602, 0.2736, 0.2001],\n",
      "          [0.3101, 0.2666, 0.3069,  ..., 0.2571, 0.2421, 0.2290],\n",
      "          [0.1121, 0.1076, 0.1447,  ..., 0.2647, 0.2581, 0.2173]],\n",
      "\n",
      "         [[0.4629, 0.4639, 0.4365,  ..., 0.5191, 0.5281, 0.5180],\n",
      "          [0.4736, 0.5012, 0.4514,  ..., 0.4760, 0.4903, 0.5290],\n",
      "          [0.4327, 0.4882, 0.5156,  ..., 0.5367, 0.4961, 0.4939],\n",
      "          ...,\n",
      "          [0.4508, 0.4411, 0.4653,  ..., 0.4099, 0.4036, 0.3485],\n",
      "          [0.4362, 0.3948, 0.4409,  ..., 0.4068, 0.4036, 0.3827],\n",
      "          [0.1488, 0.1574, 0.2106,  ..., 0.4144, 0.4180, 0.3715]]]])\n",
      "loop 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 763 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#ch0000021?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#ch0000021?line=18'>19</a>\u001b[0m _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#ch0000021?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#ch0000021?line=20'>21</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ian/Documents/researchProject/imageEnhancer/main.ipynb#ch0000021?line=21'>22</a>\u001b[0m running_corrects \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(preds \u001b[39m==\u001b[39m labels\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1147\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1146\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1148\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/loss.py:1166\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1167\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1168\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/functional.py:3012\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3011\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3012\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 763 is out of bounds."
     ]
    }
   ],
   "source": [
    "#Switch Model into eval mode\n",
    "model.eval()\n",
    "test_loader = DataLoader(testData, batch_size=50, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        print(\"label = \", labels)\n",
    "        print(\"inputs = \", inputs)\n",
    "        print(\"loop \" + str(i))\n",
    "        if (i == 50):\n",
    "            break\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        if i == 0:\n",
    "            print('======>RESULTS<======')\n",
    "            #images = torchvision.utils.make_grid(inputs[:4])\n",
    "    epoch_loss = running_loss / len(testData)\n",
    "    epoch_acc = running_corrects / len(testData) * 100.\n",
    "    print('[Test #{}] Loss: {:.4f} Acc: {:.4f}%'.\n",
    "          format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "#print(\"Rought Answer: {:.4f}%\".format(roughtAnswer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a779665a45765164b4f9c0a2c1367197c2c065231701c06d3ada1eb196bfe45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
